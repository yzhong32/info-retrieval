import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from collections import defaultdict
from nltk.stem import PorterStemmer


# 1 Text Data Parsing and Vocabulary Selection(15 points)


def preprocess_text(text):
    text = text.lower()
    text = text.replace('\\', ' ')
    text = re.sub(f'[{string.punctuation}]', '', text)
    text = re.sub(r'\d+', '', text)
    words = text.split()
    stop_words = set(stopwords.words('english'))
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    return ' '.join(words)

def create_vocabulary(text, top_n=200):
    stemmer = PorterStemmer()
    
    vocabulary = []
    cnt = defaultdict(int)
    for t in text:
        wds = t.split()
        for wd in wds:
            w = stemmer.stem(wd)
            cnt[w] += 1
    for k in cnt:
        vocabulary.append((k, cnt[k]))
    vocabulary.sort(reverse=True, key=lambda x: x[1])
    vocabulary = vocabulary[:top_n]
    return [v[0] for v in vocabulary]


data_path = './CharCnn_Keras/data/ag_news_csv/train.csv'
test_data_path = './CharCnn_Keras/data/ag_news_csv/test.csv'

df = pd.read_csv(data_path, names=['class', 'title', 'description']).sample(12000)
df['description'] = df['description'].apply(preprocess_text)

test_df = pd.read_csv(test_data_path, names=['class', 'title', 'description'])
test_df['description'] = test_df['description'].apply(preprocess_text)

vocabulary = create_vocabulary(df['description'])
print(vocabulary)


# 2 Document Relevance with Vector Space Basic Model (25 points)


def preprocess_queries(queries):
    stemmer = PorterStemmer()
    for i, query in enumerate(queries):
        wds = query.split()
        queries[i] = ' '.join([stemmer.stem(wd) for wd in wds])
        
    doc_freq = {}
    test_doc_freq = {}
    for query in queries:
        wds = query.split()
        for w in wds:
            doc_freq[w] = sum([1 for d in df['description'] if w in d.split()])
            test_doc_freq[w] = sum([1 for d in test_df['description'] if w in d.split()])
    return doc_freq, test_doc_freq

queries = ["olympic gold athens", "reuters stocks friday", "investment market prices"]
doc_freq, test_doc_freq = preprocess_queries(queries)


def compute_relevance_VSB(query, doc, normalization=False):
    wds = doc.split()
    score = 0
    for wd in query.split():
        if wd in wds:
            score += 1
    if normalization:
        doc_length = len(doc.split())
        if doc_length > 0:
            score /= doc_length
        else:
            score = 0
    return score


# 2.1 train set
for query in queries:
    scores = []
    for i, doc in enumerate(df['description']):
        scores.append((i, compute_relevance_VSB(query, doc)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 10:")
    for i, score in scores[:10]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print("Bottom 10:")
    for i, score in scores[-10:]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print('\n')


# 2.2 test set
for query in queries:
    scores = []
    for i, doc in enumerate(test_df['description']):
        scores.append((i, compute_relevance_VSB(query, doc)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 5:")
    for i, score in scores[:5]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print("Bottom 5:")
    for i, score in scores[-5:]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print('\n')


# 3 Document Relevance with Vector Space TF-IDF Model (40 points)


def compute_relevance_TF_IDF(query, doc, docs, N, avgdl, doc_freq, k=1.2, b=0.75, normalization=False):
    wds = doc.split()
    
    score = 0
    for w in query.split():
        freq = wds.count(w)
        tf = (freq * (k + 1)) / (freq + k * (1 - b + b * len(wds) / avgdl))
        f = doc_freq[w]
        idf = np.log((N + 1) / (f + 1))
        score += tf * idf

    if normalization:
        doc_length = len(doc.split())
        if doc_length > 0:
            score /= doc_length
        else:
            score = 0
    return score


# 3.1 train set
N = len(df['description'])
avgdl = sum([len(d.split()) for d in df['description']]) / N

for query in queries:
    scores = []
    for i, doc in enumerate(df['description']):
        scores.append((i, compute_relevance_TF_IDF(query, doc, df['description'], N, avgdl, doc_freq)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 10:")
    for i, score in scores[:10]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print("Bottom 10:")
    for i, score in scores[-10:]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print('\n')


# 3.2 test set
N = len(test_df['description'])
avgdl = sum([len(d.split()) for d in test_df['description']]) / N

for query in queries:
    scores = []
    for i, doc in enumerate(test_df['description']):
        scores.append((i, compute_relevance_TF_IDF(query, doc, test_df['description'], N, avgdl, test_doc_freq)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 5:")
    for i, score in scores[:5]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print("Bottom 5:")
    for i, score in scores[-5:]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print('\n')


# 4 Document Relevance with Word2Vec (20 points)


import gensim
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess

tokenized_descriptions = [nltk.word_tokenize(text) for text in df['description']]
model = Word2Vec(sentences=tokenized_descriptions, vector_size=100, window=5, min_count=1, workers=4)


from string import punctuation as punct
from scipy.spatial import distance

def cos_similarity(word, doc, model):
    if word not in model.wv:
        return 0
    target_vec = model.wv[word]
    scores = []

    for wd in doc.split():
        if wd in model.wv:
            wd_vec = model.wv[wd]
            scores.append(1 - distance.cosine(target_vec, wd_vec))
        else:
            scores.append(0)

    return np.mean(scores)

def compute_relevance_Word2Vec(query, doc, model, normalization=False):
    score = 0
    for wd in query.split():
        score += cos_similarity(wd, doc, model)
    if normalization:
        doc_length = len(doc.split())
        if doc_length > 0:
            score /= doc_length
        else:
            score = 0
    return score


# 4.1 train set
for query in queries:
    scores = []
    for i, doc in enumerate(df['description']):
        scores.append((i, compute_relevance_Word2Vec(query, doc, model)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 10:")
    for i, score in scores[:10]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print("Bottom 10:")
    for i, score in scores[-10:]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print('\n')


# 4.2 test set
for query in queries:
    scores = []
    for i, doc in enumerate(test_df['description']):
        scores.append((i, compute_relevance_Word2Vec(query, doc, model)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 5:")
    for i, score in scores[:5]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print("Bottom 5:")
    for i, score in scores[-5:]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print('\n')


# 5 Document Relevance with Vector Space (20 points)


# 5.1 Document Relevance with Vector Space Basic Model with Document Length Normalization


# 5.1.1 train set
for query in queries:
    scores = []
    for i, doc in enumerate(df['description']):
        scores.append((i, compute_relevance_VSB(query, doc, normalization=True)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 10:")
    for i, score in scores[:10]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print("Bottom 10:")
    for i, score in scores[-10:]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print('\n')


# 5.1.2 test set
for query in queries:
    scores = []
    for i, doc in enumerate(test_df['description']):
        scores.append((i, compute_relevance_VSB(query, doc)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 5:")
    for i, score in scores[:5]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print("Bottom 5:")
    for i, score in scores[-5:]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print('\n')


# 5.2 Document Relevance with Vector Space TF-IDF Model with Document Length Normalization


# 5.2.1 train set
N = len(df['description'])
avgdl = sum([len(d.split()) for d in df['description']]) / N

for query in queries:
    scores = []
    for i, doc in enumerate(df['description']):
        scores.append((i, compute_relevance_TF_IDF(query, doc, df['description'], N, avgdl, doc_freq, normalization=True)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 10:")
    for i, score in scores[:10]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print("Bottom 10:")
    for i, score in scores[-10:]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print('\n')


# 5.2.2 test set
N = len(test_df['description'])
avgdl = sum([len(d.split()) for d in test_df['description']]) / N

for query in queries:
    scores = []
    for i, doc in enumerate(test_df['description']):
        scores.append((i, compute_relevance_TF_IDF(query, doc, test_df['description'], N, avgdl, test_doc_freq, normalization=True)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 5:")
    for i, score in scores[:5]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print("Bottom 5:")
    for i, score in scores[-5:]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print('\n')


# 5.3 Document Relevance with Word2Vec with Document Length Normalization


# 5.3.1 train set
for query in queries:
    scores = []
    for i, doc in enumerate(df['description']):
        scores.append((i, compute_relevance_Word2Vec(query, doc, model, normalization=True)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 10:")
    for i, score in scores[:10]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print("Bottom 10:")
    for i, score in scores[-10:]:
        print(f"Index: {i}, Score: {score}, Description: {df['description'].iloc[i]}")
    print('\n')


# 5.3.2 test set
for query in queries:
    scores = []
    for i, doc in enumerate(test_df['description']):
        scores.append((i, compute_relevance_Word2Vec(query, doc, model, normalization=True)))
    scores.sort(reverse=True, key=lambda x: x[1])
    print(f"Query: {query}")
    print("Top 5:")
    for i, score in scores[:5]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print("Bottom 5:")
    for i, score in scores[-5:]:
        print(f"Index: {i}, Score: {score}, Description: {test_df['description'].iloc[i]}")
    print('\n')



